[W socket.cpp:401] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:558] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:558] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[2022-06-16 04:33:56,338] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl
[2022-06-16 04:34:26,388] [INFO] [partition_parameters.py:463:__exit__] finished initializing model with 6.05B parameters
Using custom data configuration default-09a0c5a4c7e74dbb
Reusing dataset text (/home/vyomkeshj/.cache/huggingface/datasets/text/default-09a0c5a4c7e74dbb/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)
100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 866.59it/s]
Using custom data configuration default-3cb01cf7977b9a02
Reusing dataset text (/home/vyomkeshj/.cache/huggingface/datasets/text/default-3cb01cf7977b9a02/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)
100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 947.65it/s]
[2022-06-16 04:34:59,304] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown
[2022-06-16 04:34:59,325] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
Using amp half precision backend
Using /home/vyomkeshj/.cache/torch_extensions/py310_cu111 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.5138401985168457 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2022-06-16 04:35:01,010] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2022-06-16 04:35:01,024] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2022-06-16 04:35:01,025] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2022-06-16 04:35:01,025] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer
[2022-06-16 04:35:01,025] [INFO] [engine.py:1410:_configure_zero_optimizer] Initializing ZeRO Stage 3
[2022-06-16 04:35:01,032] [INFO] [stage3.py:275:__init__] Reduce bucket size 500000000
[2022-06-16 04:35:01,033] [INFO] [stage3.py:276:__init__] Prefetch bucket size 50000000
Using /home/vyomkeshj/.cache/torch_extensions/py310_cu111 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.10264825820922852 seconds
***** Running training *****
  Num examples = 6300
  Num Epochs = 2
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 591
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                    | 0/591 [00:00<?, ?it/s]
[2022-06-16 04:35:11,145] [INFO] [stage3.py:567:_setup_for_real_optimizer] optimizer state initialized
[2022-06-16 04:35:12,497] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2022-06-16 04:35:12,498] [INFO] [utils.py:829:see_memory_usage] MA 1.04 GB         Max_MA 1.81 GB         CA 2.47 GB         Max_CA 2 GB
[2022-06-16 04:35:12,498] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 146.28 GB, percent = 78.1%
[2022-06-16 04:35:12,498] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2022-06-16 04:35:12,498] [INFO] [engine.py:785:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR
[2022-06-16 04:35:12,498] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x146d6c43fca0>
[2022-06-16 04:35:12,498] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:35:12,499] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2022-06-16 04:35:12,499] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2022-06-16 04:35:12,499] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-06-16 04:35:12,499] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2022-06-16 04:35:12,499] [INFO] [config.py:1063:print]   amp_params ................... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": null,
    "exps_dir": null,
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   bfloat16_enabled ............. True
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   dump_state ................... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   fp16_enabled ................. False
[2022-06-16 04:35:12,500] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   global_rank .................. 0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 1
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   loss_scale ................... 1.0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   optimizer_name ............... adamw
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   pld_params ................... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupLR
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 100}
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   steps_per_print .............. 10
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2022-06-16 04:35:12,501] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   tensorboard_output_path ......
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   train_batch_size ............. 16
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  4
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   world_size ................... 4
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 3,
    "contiguous_gradients": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 5.000000e+08,
    "allgather_partitions": true,
    "allgather_bucket_size": 5.000000e+08,
    "overlap_comm": true,
    "load_from_fp32_weights": true,
    "elastic_checkpoint": false,
    "offload_param": {
        "device": "cpu",
        "nvme_path": null,
        "buffer_count": 5,
        "buffer_size": 1.000000e+08,
        "max_in_cpu": 1.000000e+09,
        "pin_memory": false
    },
    "offload_optimizer": {
        "device": "cpu",
        "nvme_path": null,
        "buffer_count": 4,
        "pin_memory": false,
        "pipeline_read": false,
        "pipeline_write": false,
        "fast_init": false,
        "pipeline": false
    },
    "sub_group_size": 1.000000e+09,
    "prefetch_bucket_size": 5.000000e+07,
    "param_persistence_threshold": 1.000000e+05,
    "max_live_parameters": 1.000000e+09,
    "max_reuse_distance": 1.000000e+09,
    "gather_16bit_weights_on_model_save": false,
    "ignore_unused_parameters": true,
    "round_robin_gradients": false,
    "legacy_stage1": false
}
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2022-06-16 04:35:12,502] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 3
[2022-06-16 04:35:12,502] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 16,
    "bf16": {
        "enabled": true,
        "min_loss_scale": 1,
        "opt_level": "O3"
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu"
        },
        "offload_optimizer": {
            "device": "cpu"
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 5.000000e+08,
        "contiguous_gradients": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 5e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 5e-05,
            "warmup_num_steps": 100
        }
    }
}
Using /home/vyomkeshj/.cache/torch_extensions/py310_cu111 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005841255187988281 seconds


  0%|▏                                         | 2/591 [00:34<2:46:05, 16.92s/it]
{'loss': 2.3457, 'learning_rate': 7.525749891599529e-06, 'epoch': 0.01}

  1%|▏                                         | 3/591 [00:50<2:42:17, 16.56s/it]


  1%|▎                                         | 5/591 [01:24<2:42:54, 16.68s/it]

  1%|▍                                         | 6/591 [01:39<2:39:55, 16.40s/it]

  1%|▍                                         | 7/591 [01:56<2:38:58, 16.33s/it]
{'loss': 1.6777, 'learning_rate': 2.1127451000356418e-05, 'epoch': 0.02}

  1%|▌                                         | 8/591 [02:12<2:38:41, 16.33s/it]


  2%|▋                                        | 10/591 [02:44<2:37:22, 16.25s/it]
[2022-06-16 04:37:57,312] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[2.4999999999999998e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:37:57,313] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=0.9813784419605509, MemAllocated=1.04GB, MaxMemAllocated=7.84GB
{'loss': 1.0039, 'learning_rate': 2.4999999999999998e-05, 'epoch': 0.03}


  2%|▊                                        | 12/591 [03:19<2:41:28, 16.73s/it]
{'loss': 0.8242, 'learning_rate': 2.6979531151190617e-05, 'epoch': 0.03}


  2%|▉                                        | 14/591 [03:52<2:39:09, 16.55s/it]
{'loss': 0.6763, 'learning_rate': 2.8653200891955945e-05, 'epoch': 0.04}


  3%|█                                        | 16/591 [04:25<2:38:29, 16.54s/it]
{'loss': 0.7939, 'learning_rate': 3.0102999566398115e-05, 'epoch': 0.04}


  3%|█▏                                       | 18/591 [04:57<2:37:07, 16.45s/it]

  3%|█▎                                       | 19/591 [05:14<2:36:09, 16.38s/it]

  3%|█▍                                       | 20/591 [05:30<2:37:23, 16.54s/it]
[2022-06-16 04:40:43,424] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[3.2525749891599525e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:40:43,425] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=0.9727036651918666, MemAllocated=1.04GB, MaxMemAllocated=7.84GB
{'loss': 0.5251, 'learning_rate': 3.2525749891599525e-05, 'epoch': 0.05}

  4%|█▍                                       | 21/591 [05:48<2:38:45, 16.71s/it]

  4%|█▌                                       | 22/591 [06:03<2:35:35, 16.41s/it]

  4%|█▌                                       | 23/591 [06:20<2:37:05, 16.59s/it]


  4%|█▋                                       | 25/591 [06:53<2:36:10, 16.56s/it]

  4%|█▊                                       | 26/591 [07:10<2:37:14, 16.70s/it]
{'loss': 0.6165, 'learning_rate': 3.537433369927044e-05, 'epoch': 0.07}


  5%|█▉                                       | 28/591 [07:44<2:36:30, 16.68s/it]
{'loss': 0.4189, 'learning_rate': 3.6178950783555475e-05, 'epoch': 0.07}

  5%|██                                       | 29/591 [08:01<2:37:59, 16.87s/it]
[2022-06-16 04:43:30,421] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[3.6928031367991554e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:43:30,421] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=0.9683067477938155, MemAllocated=1.04GB, MaxMemAllocated=7.84GB


  5%|██▏                                      | 31/591 [08:35<2:36:49, 16.80s/it]

  5%|██▏                                      | 32/591 [08:52<2:37:06, 16.86s/it]

  6%|██▎                                      | 33/591 [09:09<2:37:17, 16.91s/it]
{'loss': 0.3575, 'learning_rate': 3.796284849694718e-05, 'epoch': 0.08}


  6%|██▍                                      | 35/591 [09:42<2:36:32, 16.89s/it]
{'loss': 0.2772, 'learning_rate': 3.8601701108756885e-05, 'epoch': 0.09}

  6%|██▍                                      | 36/591 [09:59<2:34:36, 16.71s/it]


  6%|██▋                                      | 38/591 [10:33<2:34:42, 16.79s/it]

  7%|██▋                                      | 39/591 [10:49<2:34:23, 16.78s/it]

  7%|██▊                                      | 40/591 [11:06<2:34:13, 16.79s/it]
[2022-06-16 04:46:19,076] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[4.005149978319905e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:46:19,076] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=0.9637137240592398, MemAllocated=1.04GB, MaxMemAllocated=7.84GB

  7%|██▊                                      | 41/591 [11:22<2:32:37, 16.65s/it]
{'loss': 0.2466, 'learning_rate': 4.031959641799338e-05, 'epoch': 0.1}


  7%|██▉                                      | 43/591 [11:57<2:35:20, 17.01s/it]
{'loss': 0.6096, 'learning_rate': 4.0836711389489654e-05, 'epoch': 0.11}


  8%|███                                      | 45/591 [12:32<2:37:11, 17.27s/it]

  8%|███▏                                     | 46/591 [12:49<2:37:36, 17.35s/it]
{'loss': 0.4316, 'learning_rate': 4.156894579203935e-05, 'epoch': 0.12}


  8%|███▎                                     | 48/591 [13:24<2:35:51, 17.22s/it]

  8%|███▍                                     | 49/591 [13:40<2:32:40, 16.90s/it]
{'loss': 0.3027, 'learning_rate': 4.2254902000712836e-05, 'epoch': 0.12}
[2022-06-16 04:49:09,691] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[4.247425010840046e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:49:09,713] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=0.9586338486465362, MemAllocated=1.04GB, MaxMemAllocated=7.84GB

  8%|███▍                                     | 50/591 [13:57<2:32:31, 16.92s/it]

  9%|███▌                                     | 51/591 [14:13<2:31:39, 16.85s/it]

  9%|███▌                                     | 52/591 [14:30<2:30:49, 16.79s/it]


  9%|███▋                                     | 54/591 [15:03<2:28:01, 16.54s/it]

  9%|███▊                                     | 55/591 [15:20<2:28:34, 16.63s/it]

  9%|███▉                                     | 56/591 [15:37<2:29:31, 16.77s/it]

 10%|███▉                                     | 57/591 [15:53<2:28:19, 16.67s/it]
{'loss': 0.2811, 'learning_rate': 4.3896871391812285e-05, 'epoch': 0.14}


 10%|████                                     | 59/591 [16:26<2:27:59, 16.69s/it]

 10%|████▏                                    | 60/591 [16:43<2:28:29, 16.78s/it]
[2022-06-16 04:51:56,433] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=0, lr=[4.445378125959108e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:51:56,433] [INFO] [timer.py:193:stop] 0/60, SamplesPerSec=0.9592902877610398, MemAllocated=1.04GB, MaxMemAllocated=7.84GB
{'loss': 0.2021, 'learning_rate': 4.445378125959108e-05, 'epoch': 0.15}


 10%|████▎                                    | 62/591 [17:18<2:29:57, 17.01s/it]
{'loss': 0.2197, 'learning_rate': 4.4809792237456346e-05, 'epoch': 0.16}

 11%|████▎                                    | 63/591 [17:35<2:30:49, 17.14s/it]


 11%|████▌                                    | 65/591 [18:09<2:28:37, 16.95s/it]

 11%|████▌                                    | 66/591 [18:26<2:27:12, 16.82s/it]

 11%|████▋                                    | 67/591 [18:43<2:27:20, 16.87s/it]
{'loss': 0.2047, 'learning_rate': 4.565187006752065e-05, 'epoch': 0.17}


 12%|████▊                                    | 69/591 [19:16<2:26:31, 16.84s/it]
{'loss': 0.2168, 'learning_rate': 4.597122726843138e-05, 'epoch': 0.18}
[2022-06-16 04:54:45,915] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=0, lr=[4.612745100035642e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:54:45,916] [INFO] [timer.py:193:stop] 0/70, SamplesPerSec=0.9573318980655636, MemAllocated=1.04GB, MaxMemAllocated=7.84GB


 12%|████▉                                    | 71/591 [19:49<2:25:05, 16.74s/it]

 12%|████▉                                    | 72/591 [20:06<2:25:21, 16.81s/it]
{'loss': 0.2356, 'learning_rate': 4.643331241078171e-05, 'epoch': 0.18}

 12%|█████                                    | 73/591 [20:24<2:25:48, 16.89s/it]

 13%|█████▏                                   | 74/591 [20:39<2:22:53, 16.58s/it]


 13%|█████▎                                   | 76/591 [21:13<2:22:58, 16.66s/it]
{'loss': 0.2117, 'learning_rate': 4.702033980701978e-05, 'epoch': 0.19}

 13%|█████▎                                   | 77/591 [21:30<2:23:11, 16.72s/it]

 13%|█████▍                                   | 78/591 [21:46<2:22:14, 16.64s/it]


 14%|█████▌                                   | 80/591 [22:20<2:21:35, 16.63s/it]
[2022-06-16 04:57:32,636] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=0, lr=[4.757724967479858e-05], mom=[[0.9, 0.999]]
[2022-06-16 04:57:32,637] [INFO] [timer.py:193:stop] 0/80, SamplesPerSec=0.9579717354301956, MemAllocated=1.04GB, MaxMemAllocated=7.84GB
{'loss': 0.2578, 'learning_rate': 4.757724967479858e-05, 'epoch': 0.2}

 14%|█████▌                                   | 81/591 [22:37<2:22:12, 16.73s/it]


 14%|█████▊                                   | 83/591 [23:10<2:21:58, 16.77s/it]
{'loss': 0.22, 'learning_rate': 4.7976952309401844e-05, 'epoch': 0.21}

 14%|█████▊                                   | 84/591 [23:27<2:21:12, 16.71s/it]

 14%|█████▉                                   | 85/591 [23:43<2:19:47, 16.58s/it]


 15%|██████                                   | 87/591 [24:16<2:19:06, 16.56s/it]

 15%|██████                                   | 88/591 [24:33<2:20:00, 16.70s/it]

 15%|██████▏                                  | 89/591 [24:50<2:19:00, 16.61s/it]
{'loss': 0.2856, 'learning_rate': 4.873475016612281e-05, 'epoch': 0.23}
[2022-06-16 05:00:18,449] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=0, lr=[4.885606273598312e-05], mom=[[0.9, 0.999]]
[2022-06-16 05:00:18,450] [INFO] [timer.py:193:stop] 0/90, SamplesPerSec=0.9590744650670231, MemAllocated=1.04GB, MaxMemAllocated=7.84GB


 15%|██████▎                                  | 91/591 [25:22<2:15:47, 16.29s/it]

 16%|██████▍                                  | 92/591 [25:38<2:16:25, 16.40s/it]

 16%|██████▍                                  | 93/591 [25:56<2:18:21, 16.67s/it]

 16%|██████▌                                  | 94/591 [26:12<2:18:38, 16.74s/it]
{'loss': 0.1809, 'learning_rate': 4.932819633999246e-05, 'epoch': 0.24}

 16%|██████▌                                  | 95/591 [26:29<2:18:21, 16.74s/it]


 16%|██████▋                                  | 97/591 [27:03<2:18:45, 16.85s/it]
{'loss': 0.248, 'learning_rate': 4.9669293356656114e-05, 'epoch': 0.25}

 17%|██████▊                                  | 98/591 [27:20<2:17:50, 16.78s/it]

 17%|██████▊                                  | 99/591 [27:36<2:16:04, 16.59s/it]Traceback (most recent call last):
  File "/scratch/project/dd-21-23/test/gpt-neo-fine-tuning-example/./gpt_j_deepspeed.py", line 68, in <module>
    'labels': torch.stack([f[0] for f in data])}).train()
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/transformers/trainer.py", line 1325, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/transformers/trainer.py", line 1900, in training_step
    loss = self.deepspeed.backward(loss)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1726, in backward
    self.optimizer.backward(loss)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2536, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 51, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/wandb/wandb_torch.py", line 264, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/wandb/wandb_torch.py", line 262, in _callback
    self.log_tensor_stats(grad.data, name)
  File "/home/vyomkeshj/.conda/envs/ml_test/lib/python3.10/site-packages/wandb/wandb_torch.py", line 213, in log_tensor_stats
    tensor = flat.histc(bins=self._num_bins, min=tmin, max=tmax)
RuntimeError: "histogram_cpu" not implemented for 'BFloat16'